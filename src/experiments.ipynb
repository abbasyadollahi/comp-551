{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from features import PreprocessData\n",
    "from linear_regression import linear_closed_form, linear_gradient_descent\n",
    "\n",
    "ppd = PreprocessData()\n",
    "\n",
    "# Split dataset\n",
    "train, validation, test = ppd.preprocess_data(ppd.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute most common words from \n",
    "ppd.compute_most_common_words(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "### 1 - Compare runtime, stability and performance of closed-form and gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute features on training set\n",
    "start = time.time()\n",
    "X_train, y_train = ppd.compute_features(train, simple=True)\n",
    "feat_train_runtime = time.time() - start\n",
    "\n",
    "# Compute features on validation set\n",
    "start = time.time()\n",
    "X_valid, y_valid = ppd.compute_features(validation, simple=True)\n",
    "feat_valid_runtime = time.time() - start\n",
    "\n",
    "def test_closed_vs_gradient(X_train, y_train, X_valid, y_valid, hyperparams, rand_init=True):\n",
    "    # Train using closed form method\n",
    "    start = time.time()\n",
    "    w_closed = linear_closed_form(X_train, y_train)\n",
    "    w_closed_runtime = time.time() - start\n",
    "    \n",
    "    # Train using gradient descent\n",
    "    # Hyperparameters\n",
    "    w_init = np.random.rand(X_train.shape[1]) if rand_init else np.zeros(X_train.shape[1])\n",
    "    decay_speed = hyperparams['decay_speed']\n",
    "    learn_rate = hyperparams['learn_rate']\n",
    "    min_err = hyperparams['min_err']\n",
    "    max_iter = hyperparams['max_iter']\n",
    "\n",
    "    start = time.time()\n",
    "    w_grad = linear_gradient_descent(X_train, y_train, w_init, decay_speed, learn_rate, min_err, max_iter)\n",
    "    w_grad_runtime = time.time() - start\n",
    "    \n",
    "    # Compute MSE on training set\n",
    "    y_closed_train = np.matmul(X_train, w_closed)\n",
    "    mse_closed_train = np.sum((y_closed_train - y_train)**2)/len(y_train)\n",
    "\n",
    "    y_grad_train = np.matmul(X_train, w_grad)\n",
    "    mse_grad_train = np.sum((y_grad_train - y_train)**2)/len(y_train)\n",
    "    \n",
    "    # Compute MSE on validation set\n",
    "    y_closed_valid = np.matmul(X_valid, w_closed)\n",
    "    mse_closed_valid = np.sum((y_closed_valid - y_valid)**2)/len(y_valid)\n",
    "\n",
    "    y_grad_valid = np.matmul(X_valid, w_grad)\n",
    "    mse_grad_valid = np.sum((y_grad_valid - y_valid)**2)/len(y_valid)\n",
    "    \n",
    "    return {'train': {'closed': mse_closed_train, 'grad': mse_grad_train}, 'validation': {'closed': mse_closed_valid, 'grad': mse_grad_valid}, 'runtime': {'closed': w_closed_runtime, 'grad': w_grad_runtime}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Testing random w0 vs. zero w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexh/comp551/src/linear_regression.py:17: RuntimeWarning: invalid value encountered in subtract\n",
      "  w_curr = w_prev - 2*curr_learn_rate*(np.matmul(xtx_product, w_prev) - xty_product)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: nan | Learning rate: 9.9999990000001e-05\n",
      "Error: nan | Learning rate: 9.999998000000401e-05\n",
      "Error: nan | Learning rate: 9.999997000000902e-05\n",
      "Error: nan | Learning rate: 9.999996000001601e-05\n",
      "Error: nan | Learning rate: 9.999995000002499e-05\n",
      "Error: nan | Learning rate: 9.999994000003601e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0768d7fb99d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mhyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'decay_speed'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'learn_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'min_err'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_iter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10000000\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mperf_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_closed_vs_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mperf_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_closed_vs_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperf_rand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperf_zero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-47091eaea2c5>\u001b[0m in \u001b[0;36mtest_closed_vs_gradient\u001b[0;34m(X_train, y_train, X_valid, y_valid, hyperparams, rand_init)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mw_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_speed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mw_grad_runtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/comp551/src/linear_regression.py\u001b[0m in \u001b[0;36mlinear_gradient_descent\u001b[0;34m(X, y, w_init, decay_speed, learn_rate, min_err, max_iter)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mcurr_learn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdecay_speed\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mw_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_prev\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcurr_learn_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtx_product\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mxty_product\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_curr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mw_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_err\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_iter\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparams = {'decay_speed': 10**(-12), 'learn_rate': 10**(-4), 'min_err': 10**(-7), 'max_iter': 10000000}\n",
    "perf_rand = test_closed_vs_gradient(X_train, y_train, X_valid, y_valid, hyperparams, rand_init=True)\n",
    "perf_zero = test_closed_vs_gradient(X_train, y_train, X_valid, y_valid, hyperparams, rand_init=False)\n",
    "print(perf_rand)\n",
    "print(perf_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615.7754010695188\n",
      "63.18164634722616\n"
     ]
    }
   ],
   "source": [
    "print(((perf_rand['runtime']['closed'] - perf_zero['runtime']['closed'])/perf_zero['runtime']['closed'])*100)\n",
    "print(((perf_rand['runtime']['grad'] - perf_zero['runtime']['grad'])/perf_zero['runtime']['grad'])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime improvement on closed form method when using random initialization of weights vector is 2920% compared to zero initialization. As for the gradient descent method, the impact is much smaller at only 12%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
